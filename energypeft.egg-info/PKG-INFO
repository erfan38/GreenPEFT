Metadata-Version: 2.1
Name: energypeft
Version: 0.1.0
Summary: Energy-Aware Parameter-Efficient Fine-Tuning Framework
Home-page: https://github.com/erfan38/energypeft
Author: Fatemeh Erfan
Author-email: fatemeh.erfan@polymtl.ca
Keywords: machine-learning,deep-learning,energy-efficiency,parameter-efficient-fine-tuning,peft,sustainable-ai,llm,transformers,energy-monitoring
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: System :: Monitoring
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.0.0
Requires-Dist: transformers>=4.30.0
Requires-Dist: peft>=0.4.0
Requires-Dist: datasets>=2.10.0
Requires-Dist: pynvml>=11.4.1
Requires-Dist: psutil>=5.9.0
Requires-Dist: codecarbon>=2.1.4
Requires-Dist: numpy>=1.21.0
Requires-Dist: pandas>=1.3.0
Requires-Dist: tqdm>=4.62.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: isort>=5.10.0; extra == "dev"
Requires-Dist: flake8>=4.0.0; extra == "dev"
Requires-Dist: mypy>=0.950; extra == "dev"
Provides-Extra: examples
Requires-Dist: jupyter>=1.0.0; extra == "examples"
Requires-Dist: matplotlib>=3.5.0; extra == "examples"
Requires-Dist: seaborn>=0.11.0; extra == "examples"

# EnergyPEFT: Energy-Aware Parameter-Efficient Fine-Tuning Framework

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/release/python-380/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Energy Efficient](https://img.shields.io/badge/AI-Energy%20Efficient-green.svg)](https://github.com/yourusername/energypeft)

**EnergyPEFT** is a comprehensive framework for energy-aware parameter-efficient fine-tuning (PEFT) of large language models. It provides real-time energy monitoring, carbon emission tracking, and intelligent optimization strategies to make AI model training more sustainable and cost-effective.

## ğŸŒ± Key Features

- **âš¡ 1- Real-time Energy Monitoring**: Track GPU and CPU energy consumption during training
- **ğŸ§  2- Smart Sampling**: Gradient-based importance sampling for efficient data usage
- **ğŸ“Š 3- Adaptive Batching**: Dynamic batch size optimization based on energy constraints
- **ğŸ›‘ 4- Early Stopping**: Energy-efficiency-based stopping criteria
- **ğŸ’° 5- Budget Allocation**: Strategic energy budget management across training phases
- **ğŸŒ 6- Carbon Tracking**: Monitor and minimize CO2 emissions during training
- **ğŸ”§ 7- Multiple Integrations**: Support for HuggingFace PEFT, LlamaFactory, and direct Transformers
- **ğŸ¯ 8- Drop-in Trainer**: GreenTrainer as a direct replacement for transformers.Trainer

## ğŸ“ Project Structure

```

Green PEFT/                                    # Root project directory
â”œâ”€â”€ setup.py                                  # Package installation configuration
â”œâ”€â”€ README.md                                 # Project documentation  
â”œâ”€â”€ requirements.txt                          # Python dependencies
â”œâ”€â”€ .gitignore                               # Git ignore rules
â”œâ”€â”€ quick_test.py
â”œâ”€â”€ llamafactory_example.py
â”œâ”€â”€test_energy_peft_installation.py
â”‚
â”œâ”€â”€ energypeft/                              # Main Python package
â”‚   â”œâ”€â”€ __init__.py                         # Package entry point & main API
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                               # Core energy optimization components (moved 
â”‚   â”‚   â”œâ”€â”€ __init__.py                    # Core module exports
â”‚   â”‚   â”œâ”€â”€ energy_monitor.py              # Step 1: Real-time monitoring
â”‚   â”‚   â”œâ”€â”€ smart_sampler.py               # Step 2,4: Energy-constrained sampling: Gradient importance sampling  
â”‚   â”‚   â”œâ”€â”€ adaptive_batcher.py            # Step 3: Dynamic batch sizing
â”‚   â”‚   â””â”€â”€ early_stopper.py               # Step 5: Energy-efficiency stopping
â”‚   â”‚
â”‚   â”œâ”€â”€ integrations/                       # Framework integrations
â”‚   â”‚   â”œâ”€â”€ __init__.py                    # Integration module exports
â”‚   â”‚   â”œâ”€â”€ huggingface_peft.py            # HuggingFace PEFT integration
â”‚   â”‚   â”œâ”€â”€ llamafactory.py                # LlamaFactory wrapper with energy awareness
â”‚   â”‚   â””â”€â”€ transformers.py                # Direct transformers integration
â”‚   â”‚
â”‚   â”œâ”€â”€ trainers/                           # Energy-aware trainer classes 
â”‚   â”‚   â”œâ”€â”€ __init__.py                    # Trainer module exports  
â”‚   â”‚   â””â”€â”€ green_trainer.py               # Drop-in replacement for transformers.Trainer
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                              # Utility modules
â”‚   â”‚   â”œâ”€â”€ __init__.py                    # Utils module exports
â”‚   â”‚   â”œâ”€â”€ carbon_scheduler.py            # Step 7: Carbon-aware scheduling
â”‚   â”‚   â”œâ”€â”€ budget_allocator.py            # Step 6: Strategic budget
 reports (evolved from metrics.py)
â”‚   â”‚   â””â”€â”€ hardware_profiler.py           # Hardware-specific energy profiling (NEW)
â”‚   â”‚
â”‚   â””â”€â”€ examples/                           # Usage examples & demos
â”‚       â”œâ”€â”€ __init__.py                    # Examples module exports
â”‚       â”œâ”€â”€ llamafactory_example.py        # LlamaFactory training example
â”‚       â”œâ”€â”€ lora_example.py               # LoRA fine-tuning example
â”‚
â”œâ”€â”€ tests/                                  # Test suite
â”‚   â”œâ”€â”€ __init__.py                        # Test module init
â”‚   â”œâ”€â”€ test_energy_monitor.py             # Energy monitoring tests
â”‚   â”œâ”€â”€ test_smart_sampler.py              # Smart sampling tests
â”‚   â”œâ”€â”€ test_adaptive_batcher.py           # Adaptive batching tests
â”‚   â”œâ”€â”€ test_integrations.py               # Integration tests
â”‚   â””â”€â”€ test_end_to_end.py                 # End-to-end workflow tests
â”‚
â”œâ”€â”€ docs/                                   # Documentation
â”‚   â”œâ”€â”€ index.md                           # Documentation home
â”‚   â”œâ”€â”€ installation.md                    # Installation guide
â”‚   â”œâ”€â”€ quickstart.md                      # Quick start guide
â”‚   â”œâ”€â”€ api_reference.md                   # API documentation
â”‚   â””â”€â”€ examples.md                        # Usage examples
â”‚
â””â”€â”€ scripts/                                # Development & utility scripts
    â”œâ”€â”€ benchmark_energy.py                # Energy consumption benchmarks
    â”œâ”€â”€ setup_environment.py               # Development environment setup
    â””â”€â”€ generate_reports.py                # Energy report generation
```

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/erfan38/energypeft.git
cd energypeft

# Install in development mode
pip install -e .

# Verify installation
python quick_test.py
```

### Basic Usage

#### Current API (Available Now)

```python
import energypeft
from transformers import AutoModel, AutoTokenizer
from peft import get_peft_model, LoraConfig

# Initialize EnergyPEFT framework
energy_peft = energypeft.EnergyPEFT(
    energy_budget_wh=100.0,      # 100 Watt-hours energy budget
    base_batch_size=32,          # Starting batch size
    importance_weight=0.7        # Balance between importance and diversity
)

# Load your model and tokenizer
model = AutoModel.from_pretrained("your-model-name")
tokenizer = AutoTokenizer.from_pretrained("your-model-name")

# Create PEFT model
peft_config = LoraConfig(r=16, lora_alpha=32)
model = get_peft_model(model, peft_config)

# Wrap trainer with energy awareness
trainer = energy_peft.wrap_trainer(
    trainer_type="huggingface",
    model=model,
    tokenizer=tokenizer,
    train_dataset=your_dataset
)

# Train with automatic energy optimization
trainer.train()
```

#### Future API (GreenTrainer - In Development)

```python
import energypeft
from transformers import TrainingArguments

# Drop-in replacement for transformers.Trainer
trainer = energypeft.GreenTrainer(
    model=model,
    tokenizer=tokenizer,
    args=TrainingArguments(...),
    train_dataset=dataset,
    energy_budget_wh=100.0  # Energy-aware training
)

# Train with automatic energy optimization
trainer.train()
```

## ğŸ¯ Framework Integrations

### HuggingFace PEFT Integration
```python
# Energy-aware HuggingFace PEFT training
trainer = energy_peft.wrap_trainer(
    trainer_type="huggingface",
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset
)
```

### LlamaFactory Integration
```python
# LlamaFactory with energy constraints
from energypeft.integrations import LlamaFactoryEnergyWrapper

llamafactory_wrapper = LlamaFactoryEnergyWrapper(
    energy_framework=energy_peft,
    llamafactory_config=your_config
)

# Run energy-optimized training
results = llamafactory_wrapper.train_with_energy_monitoring()
```

## ğŸ“Š Core Components

### Energy Monitor (`core/energy_monitor.py`)
Real-time tracking of:
- GPU power consumption (NVIDIA GPUs)
- CPU energy usage
- Memory utilization
- Carbon footprint estimation

### Smart Sampler (`core/smart_sampler.py`)
- **Gradient-based importance scoring**: Identifies most valuable training samples
- **Energy-constrained sampling**: Adapts sample selection based on energy budget
- **Without-replacement sampling**: Ensures efficient data coverage

### Adaptive Batcher (`core/adaptive_batcher.py`)
- **Dynamic batch sizing**: Adjusts batch size based on remaining energy
- **Progress-aware optimization**: Smaller batches in later training phases
- **Memory-conscious adaptation**: Prevents OOM errors

### Energy-Efficiency Early Stopping (`core/early_stopper.py`)
- **Energy efficiency metrics**: Stops training when efficiency drops
- **Budget-aware termination**: Prevents energy budget overrun
- **Configurable patience**: Balances efficiency with training completeness

### Budget Allocator (`utils/budget_allocator.py`)
- **Strategic energy allocation**: Distributes energy budget across training phases
- **Phase-aware optimization**: Adapts energy usage for different training stages
- **Resource prioritization**: Focuses energy on most important training steps

### Carbon Scheduler (`utils/carbon_scheduler.py`)
- **CO2 emission tracking**: Real-time carbon footprint monitoring
- **Carbon-aware scheduling**: Optimizes training timing for lower emissions
- **Sustainability metrics**: Comprehensive environmental impact reporting

## ğŸ”§ Configuration Options

```python
energy_peft = energypeft.EnergyPEFT(
    energy_budget_wh=150.0,           # Total energy budget in Watt-hours
    base_batch_size=16,               # Initial batch size
    importance_weight=0.8,            # Importance vs diversity balance (0-1)
    min_batch_size=1,                 # Minimum allowed batch size
    patience=5,                       # Early stopping patience
    energy_tracking_interval=10       # Energy measurement frequency (steps)
)
```

## ğŸ“ˆ Energy Reporting

EnergyPEFT provides comprehensive energy usage reports:

- **Total energy consumed** (Wh)
- **Carbon emissions** (kg CO2)
- **Energy efficiency** (samples/Wh)
- **Budget utilization** (%)
- **Cost estimation** (cloud training)
- **Sustainability metrics**

## ğŸ§ª Examples

Check the `examples/` directory for detailed usage examples:

- **`api_demo.py`**: API usage demonstrations
- **`greenpeft_usage_examples.py`**: Comprehensive usage examples
- **`lora_example.py`**: LoRA fine-tuning with energy awareness
- **Root level `llamafactory_example.py`**: LlamaFactory integration demo

## ğŸ§  Research & Development

The `src/` directory contains research notebooks and development files:
- Experimental implementations
- Research prototypes
- Performance analysis
- Development examples

The `papers/` directory contains academic documentation and research papers related to energy-efficient AI training.

## ğŸ“‹ Requirements

- Python >= 3.8
- PyTorch >= 2.0.0
- Transformers >= 4.30.0
- PEFT >= 0.4.0
- CodeCarbon >= 2.1.4
- NVIDIA GPU (recommended for energy monitoring)

## ğŸš€ Installation Test

After installation, run our comprehensive test:

```bash
# Quick functionality test
python quick_test.py

# Energy PEFT specific test
python test_energy_peft_installation.py

# Test with current API
python -c "import energypeft; print('âœ… EnergyPEFT imported successfully!')"
```

# ================================================================
# RECOMMENDED USER DOCUMENTATION
# ================================================================

"""
ğŸ“š HOW TO DOCUMENT THESE PATTERNS:

BEGINNER USERS â†’ Start with Pattern 1 (GreenTrainer)
"Just replace Trainer with GreenTrainer for automatic energy optimization!"

INTERMEDIATE USERS â†’ Use Pattern 2 (EnergyPEFT Framework)  
"Get more control over energy components while keeping it simple!"

ADVANCED USERS â†’ Use Pattern 3 (Direct Components)
"Build custom energy-aware training solutions!"

RESEARCH USERS â†’ Use Pattern 4 (PEFT Integration)
"Combine energy efficiency with parameter-efficient fine-tuning!"
"""

## ğŸ¤ Contributing

Contributions are welcome! This project aims to make AI training more sustainable and accessible. Areas for contribution:

- Additional framework integrations
- Hardware-specific optimizations
- Energy profiling improvements
- Documentation and examples
- Research on energy-efficient training methods

## ğŸ“ Research Applications

EnergyPEFT is particularly valuable for:

- **Academic research** with limited computational budgets
- **Sustainable AI development** focused on reducing carbon footprint
- **Cost-efficient fine-tuning** in cloud environments
- **Green AI research** and energy-aware machine learning
- **Smart contract vulnerability detection** and security research
- **Resource-constrained environments** with limited power availability

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Built with [HuggingFace Transformers](https://github.com/huggingface/transformers) and [PEFT](https://github.com/huggingface/peft)
- Energy monitoring powered by [CodeCarbon](https://github.com/mlco2/codecarbon)
- Inspired by the need for sustainable AI development and research

## ğŸ“š Citation

If you use EnergyPEFT in your research, please cite:

```bibtex
@software{energypeft2025,
  title={EnergyPEFT: Energy-Aware Parameter-Efficient Fine-Tuning Framework},
  author={Fatemeh Erfan},
  year={2025},
  url={https://github.com/yourusername/energypeft}
}
```

---

 **Making AI Training Sustainable, One Model at a Time** 
